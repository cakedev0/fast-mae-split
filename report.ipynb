{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129ed178",
   "metadata": {},
   "source": [
    "**TL;DR**: Maintain the weighted $\\alpha$-quantile with two heaps, update four aggregates, and evaluate pinball loss in $O(1)$ per threshold. You get $O(n\\log n)$ expected time under mild conditions, with a clean $O(n\\log n)$ worst-case alternative via a segment tree when needed.\n",
    "\n",
    "# Efficient Weighted Quantile–Based Splitting for Decision Trees\n",
    "\n",
    "\n",
    "This repository provides an efficient implementation of quantile-based impurities (pinball loss)\n",
    "for decision-tree regression with **weighted samples** and **arbitrary quantile levels** $\\alpha\\in[0,1]$.\n",
    "\n",
    "* **Two-heaps method (default):** Expected $O(n\\log n)$ per feature under mild assumptions; very fast in practice.\n",
    "  This was implemented in scikit-learn in PR [#32100](https://github.com/scikit-learn/scikit-learn/pull/32100)\n",
    "  for the absolute error case ($\\alpha = 0.5$) replacing an $O(n^2)$ implementation.\n",
    "  A future PR will generalize to **arbitrary quantile levels**.\n",
    "* **Segment tree alternative:** Worst-case $O(n\\log n)$ per feature by design; typically ~3× slower than two-heaps in benchmarks, but provides a clean theoretical upper bound.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Pinball loss with weights and an $O(1)$ formula\n",
    "\n",
    "For weighted data ${(y_i,w_i)}_{i=1}^n$ with $w_i > 0$, the pinball loss at level $\\alpha\\in[0,1]$ for a prediction $q\\in\\mathbb{R}$ is\n",
    "\n",
    "$$\n",
    "L_\\alpha(q)\n",
    "= \\sum_{i} w_i \\left(\\alpha\\max(y_i-q,0) + (1-\\alpha)\\max(q-y_i,0)\\right).\n",
    "$$\n",
    "\n",
    "Splitting by whether $y_i\\ge q$ or $y_i < q$, define the aggregates\n",
    "$$\n",
    "W^+(q)= \\sum_{y_i\\ge q} w_i,\\quad\n",
    "Y^+(q)= \\sum_{y_i\\ge q} w_i y_i,\\qquad\n",
    "W^-(q)= \\sum_{y_i< q} w_i,\\quad\n",
    "Y^-(q)= \\sum_{y_i< q} w_i y_i.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\boxed{L_\\alpha(q)\n",
    "= \\alpha\\big(Y^+(q) - q W^+(q)\\big)\n",
    "+ (1-\\alpha)\\big(q W^-(q)-Y^-(q)\\big)}\n",
    "$$\n",
    "which is **$O(1)$** to evaluate once the four aggregates are maintained.\n",
    "\n",
    "**Code (notation: `alpha` is the level, `q` is the value):**\n",
    "\n",
    "```python\n",
    "loss = (\n",
    "    alpha * (above.weighted_sum - q * above.total_weight)\n",
    "     + (1 - alpha) * (q * below.total_weight - below.weighted_sum)\n",
    ")\n",
    "# \"above\": indices with y_i >= q; \"below\": indices with y_i < q\n",
    "```\n",
    "\n",
    "$q$ is a **weighted $\\alpha$-quantile** of the current set.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Algorithms\n",
    "\n",
    "### 2.1 Two-heaps (weighted $\\alpha$-quantile maintenance)\n",
    "\n",
    "This algorithm is a weighted adaptation of the two-heaps solution of the median of a data-stream problem (see for instance this [leetcode solution](https://leetcode.com/problems/find-median-from-data-stream/solutions/7146165/o-logn-2-heaps-python))\n",
    "\n",
    "Maintain two heaps keyed by $y$: a max-heap for items below $q$ and a min-heap for items at/above $q$. Balance by **total weight** (not count) so that\n",
    "$$\n",
    "W^- \\approx \\alpha W \\quad \\text{with } W^-= \\sum_{y_i<q} w_i \\, , \\; W=W^- + W^+.\n",
    "$$\n",
    "During a left→right sweep over samples sorted by the candidate feature:\n",
    "\n",
    "1. Insert $(y_i,w_i)$ into the appropriate heap. Complexity: $O(\\log n)$\n",
    "2. **Rebalance by weight** by moving boundary items across heaps until $W^- \\approx \\alpha W$. **Expected** complexity: $O(\\log n)$, see section 3 for more details.\n",
    "3. Read off $q$ at the boundary and compute the child loss with the $O(1)$ formula.\n",
    "\n",
    "A symmetric right→left sweep yields the right-child losses; summing gives the impurity at each threshold.\n",
    "\n",
    "**Implementation:**\n",
    "- A simple python WeightedHeap (wraps stdlib's `heapq`): `lib/ds/pythonheap.py` \n",
    "- A numba WeightedHeap: `lib/ds/heap.py` \n",
    "- The left→right sweep loop: `lib/algos/heap.py` \n",
    "\n",
    "### 2.2 Segment tree (guaranteed $O(n\\log n)$)\n",
    "\n",
    "\n",
    "**Idea.** Maintain a static binary tree over the **sorted order of targets $y$**. Each leaf corresponds to one **rank** position in that sorted order (smallest $y_i$ has rank $0$ and so on); every internal node stores two aggregates for its subtree:\n",
    "\n",
    "* total weight $W$ and\n",
    "* total weighted target $Y=\\sum w_i y_i$.\n",
    "\n",
    "The tree is initialized with all aggregates set to zero—that is, all leaves start “empty.” \n",
    "\n",
    "This lets us (i) **set** the current sample’s $(y_i, w_i)$ at its leaf (i.e. at the **rank** of $y_i$), updating $W$ and $Y$ up the path, and (ii) **search** by **cumulative weight** to find the weighted $\\alpha$-quantile and the prefix aggregates needed for the $O(1)$ pinball-loss formula.\n",
    "\n",
    "**Operations.**\n",
    "\n",
    "* **SET(rank, $w$, $y$)**: add $(w,y)$ to the leaf at `rank`; bubble updates to the root, maintaining subtree aggregates $(W,Y)$. Cost $O(\\log n)$.\n",
    "* **SEARCH($t$)**: given a weight target $t=\\alpha \\cdot W_{\\text{total}}$, descend from the root choosing left/right by comparing (t) to the left child’s weight. This finds the **leaf** where the weighted rank crosses (t); along the way you accumulate the **prefix** (W^-) and (Y^-). Return ((W^-, Y^-, q)) where (q) is the leaf’s (y)-value (the current weighted (\\alpha)-quantile). Cost (O(\\log n)).\n",
    "\n",
    "(A symmetric right→left sweep yields right-child losses; add left+right to score each threshold.)\n",
    "\n",
    "**Complexity and performance.**\n",
    "\n",
    "* Each step performs one `SET` and one `SEARCH`: **$O(\\log n)$** each ⇒ **$O(n\\log n)$** per sweep, worst-case by design.\n",
    "* In practice this variant is typically **~3× slower** than the two-heaps method on the same data (larger constants, more memory traffic), but it provides a clean upper bound and deterministic per-step work.\n",
    "\n",
    "**Implementation.**\n",
    "\n",
    "* The weighted segment tree class: `lib/ds/segment_tree.py` \n",
    "* The left→right sweep loop: `lib/algos/segment_tree.py` \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Complexity of the two-heaps algorithm\n",
    "\n",
    "See the notebook [complexity_experiments](https://github.com/cakedev0/fast-mae-split/blob/main/complexity_experiments.ipynb) for experiments and plots illustrating the different statements of this section.\n",
    "\n",
    "### 3.1 Worst-case for two-heaps\n",
    "\n",
    "In adversarial orders of weights, a single insertion can force the $\\alpha$-quantile boundary to traverse $\\Theta(n)$ items (each move is a heap pop+push, $O(\\log n)$). Thus **worst-case** per insertion is $O(n\\log n)$ and a full sweep can be **$O(n^2\\log n)$**.\n",
    "\n",
    "This does **not** occur in typical data; it requires systematically placing extreme weights to repeatedly push the boundary across many tiny items.\n",
    "\n",
    "\n",
    "### 3.2 Expected $O(n\\log n)$ per feature — two versions\n",
    "\n",
    "We give two simple sets of conditions under which the **expected** number of boundary moves per insertion is **$O(1)$**, yielding $O(n\\log n)$ total expected time (insert + moves, each $O(\\log n)$).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2A Independence model: $Y \\perp W$\n",
    "\n",
    "**Assumptions.**\n",
    "\n",
    "* $Y_1, \\dots, Y_n\\;$ i.i.d. from a continuous distribution (or consistent tie-breaking).\n",
    "* $W_1, \\dots, W_n\\;$ i.i.d., non-negative, independent of the $Y_i$, with $0<\\mu=\\mathbb{E}[W_i]<\\infty$.\n",
    "\n",
    "**Intuition.** Insert $(Y_{t+1},W_{t+1})$. The target balance changes by at most the new weight, so the boundary must “absorb” weight\n",
    "$$\n",
    "B_t\\in{\\alpha W_{t+1}, (1-\\alpha)W_{t+1}}\\le W_{t+1}.\n",
    "$$\n",
    "\n",
    "To re-balance, we move boundary items one by one. By independence and continuity, the weights encountered at the boundary look like fresh draws from $W$, so a **typical boundary item contributes $\\approx \\mu$ weight**. Hence the expected number of moves is about $\\mathbb{E}[B_t]/\\mu\\le \\mu/\\mu=O(1)$ (up to a constant for the final overshoot). Each move costs $O(\\log n)$; adding the $O(\\log n)$ insertion yields $O(\\log n)$ expected time per sample and $O(n\\log n)$ per feature.\n",
    "\n",
    "**Remark.** Heavy tails with finite mean (e.g., log-normal, Pareto with $\\alpha>1$) increase variance but not the expectation; infinite-mean tails would break the bound. However, in practice, even with infinite-mean Pareto distributions, performance does not degrade significantly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2B Lower-bound model: $f(y)=\\mathbb{E}[W\\mid Y=y]\\ge c>0$\n",
    "\n",
    "**Assumptions.**\n",
    "\n",
    "* $(Y_i,W_i)$ are i.i.d.; $\\mu=\\mathbb{E}[W_i]<\\infty$.\n",
    "* $Y$ has a continuous distribution (or consistent tie-breaking).\n",
    "* There exists $c>0$ such that for all $y$ in the support of $Y$,\n",
    "  $$\n",
    "  f(y):=\\mathbb{E}[W\\mid Y=y]\\ \\ge\\ c.\n",
    "  $$\n",
    "\n",
    "**Intuition.** As above, each insertion requires shifting at most $B_t\\le W_{t+1}$ weight across the boundary. The items encountered while sliding the boundary have an **expected weight of at least $c$**. Therefore, the **expected number of moves** satisfies\n",
    "$$\n",
    "\\mathbb{E}[K_t]\\ \\lesssim\\ \\frac{\\mathbb{E}[B_t]}{c} + 1 \\ \\le\\ \\frac{\\mu}{c}+1 \\ =\\ O(1).\n",
    "$$\n",
    "\n",
    "Again, with $O(\\log n)$ per move and insertion, we obtain $O(n\\log n)$ expected time, but with possibly a much bigger constant than above, if $\\frac{\\mu}{c}$ is big. See graph *small_weights_around_median* in `complexity_experiments.ipynb` for an example of such a setting.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Empirical performance (illustrative)\n",
    "\n",
    "* **Setup:** $n=100{,}000$\n",
    "* **Previous $O(n^2)$ MAE path:** $\\sim 10$ s.\n",
    "* **Two-heaps:** $\\sim 0.2$ s.\n",
    "* **Segment tree:** typically ~3× slower than two-heaps on the same data, expect for adversial input,\n",
    "  or low mean around quantile case: $\\mathbb{E}[W|y \\approx q] << \\mathbb{E}[W]$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Related work (brief)\n",
    "\n",
    "This approach adapts the classic two-heaps streaming median to **weighted $\\alpha$-quantiles** and couples it with a simple $O(1)$ pinball-loss evaluation. Some [prior work](https://faculty.ucmerced.edu/hbhat/BhatKumarVaz_final.pdf) also proposes a two-heaps algorithm for quantile-regression trees, but it omits weights and rely on more elaborate derivations for pinball-loss evaluation ($O(1)$ too, but might not be easy to adapt for the weighted case). The present formulation is simple and easy to integrate into open-source libraries such as scikit-learn.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
