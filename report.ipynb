{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bb2da5",
   "metadata": {},
   "source": [
    "# Efficient Weighted Quantile–Based Splitting for Decision Trees\n",
    "\n",
    "\n",
    "This repository provides an efficient implementation of quantile-based impurities (pinball loss)\n",
    "for decision-tree regression with **weighted samples** and **arbitrary quantile levels** $\\alpha\\in[0,1]$.\n",
    "\n",
    "* **Two-heaps method (default):** Expected $O(n\\log n)$ per feature under mild assumptions; very fast in practice.\n",
    "  This was implemented initially implemented in scikit-learn PR [#32100](https://github.com/scikit-learn/scikit-learn/pull/32100)\n",
    "  for the absolute error case ($\\alpha = 0.5$) replacing an $O(n^2)$ implementation.\n",
    "* **Segment tree and Fenwick tree alternatives:** Both provide a strict $O(n\\log n)$ worst-case complexity by design.\n",
    "  Both are very similar, but the Fenwick tree method is quite faster in practice. Initially though slower, **practical performances of the Fenwick\n",
    "  method was found comparable with the two-heaps method, and hence was ultimately chosen to be implement in scikit-learn.**\n",
    "\n",
    "A future PR will generalize to **arbitrary quantile levels**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f511af",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Pinball loss with weights and an $O(1)$ formula\n",
    "\n",
    "For weighted data ${(y_i,w_i)}_{i=1}^n$ with $w_i > 0$, the pinball loss at level $\\alpha\\in[0,1]$ for a prediction $q\\in\\mathbb{R}$ is\n",
    "\n",
    "$$\n",
    "L_\\alpha(q)\n",
    "= \\sum_{i} w_i \\left(\\alpha\\max(y_i-q,0) + (1-\\alpha)\\max(q-y_i,0)\\right).\n",
    "$$\n",
    "\n",
    "Splitting by whether $y_i\\ge q$ or $y_i < q$, define the aggregates\n",
    "$$\n",
    "W^+(q)= \\sum_{y_i\\ge q} w_i,\\quad\n",
    "Y^+(q)= \\sum_{y_i\\ge q} w_i y_i,\\qquad\n",
    "W^-(q)= \\sum_{y_i< q} w_i,\\quad\n",
    "Y^-(q)= \\sum_{y_i< q} w_i y_i.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\boxed{L_\\alpha(q)\n",
    "= \\alpha\\big(Y^+(q) - q W^+(q)\\big)\n",
    "+ (1-\\alpha)\\big(q W^-(q)-Y^-(q)\\big)}\n",
    "$$\n",
    "which is **$O(1)$** to evaluate once the four aggregates are maintained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ed178",
   "metadata": {},
   "source": [
    "## 2. Algorithms\n",
    "\n",
    "### 2.1 Two-heaps (weighted $\\alpha$-quantile maintenance)\n",
    "\n",
    "This algorithm is a weighted adaptation of the two-heaps solution of the median of a data-stream problem (see for instance this [leetcode solution](https://leetcode.com/problems/find-median-from-data-stream/solutions/7146165/o-logn-2-heaps-python))\n",
    "\n",
    "Maintain two heaps keyed by $y$: a max-heap for items below $q$ and a min-heap for items at/above $q$. Balance by **total weight** (not count) so that\n",
    "$$\n",
    "W^- \\approx \\alpha W \\quad \\text{with } W^-= \\sum_{y_i<q} w_i \\, , \\; W=W^- + W^+.\n",
    "$$\n",
    "During a left→right sweep over samples sorted by the candidate feature:\n",
    "\n",
    "1. Insert $(y_i,w_i)$ into the appropriate heap. Complexity: $O(\\log n)$\n",
    "2. **Rebalance by weight** by moving boundary items across heaps until $W^- \\approx \\alpha W$. **Expected** complexity: $O(\\log n)$, see section 3 for more details.\n",
    "3. Read off $q$ at the boundary and compute the child loss with the $O(1)$ formula.\n",
    "\n",
    "A symmetric right→left sweep yields the right-child losses; summing gives the impurity at each threshold.\n",
    "\n",
    "**Implementation:**\n",
    "- A simple python WeightedHeap (wraps stdlib's `heapq`): `lib/ds/pythonheap.py` \n",
    "- A numba WeightedHeap: `lib/ds/heap.py` \n",
    "- The left→right sweep loop: `lib/algos/heap.py` \n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Segment tree (guaranteed $O(n\\log n)$)\n",
    "\n",
    "\n",
    "**Idea.** Maintain a static binary tree over the **sorted order of targets $y$**. Each leaf corresponds to one **rank** position in that sorted order (smallest $y_i$ has rank $0$ and so on); every internal node stores two aggregates for its subtree:\n",
    "\n",
    "* total weight $W$ and\n",
    "* total weighted target $Y=\\sum w_i y_i$.\n",
    "\n",
    "The tree is initialized with all aggregates set to zero—that is, all leaves start “empty.” \n",
    "\n",
    "This lets us (i) **set** the current sample’s $(y_i, w_i)$ at its leaf (i.e. at the **rank** of $y_i$), updating $W$ and $Y$ up the path, and (ii) **search** by **cumulative weight** to find the weighted $\\alpha$-quantile and the prefix aggregates needed for the $O(1)$ pinball-loss formula.\n",
    "\n",
    "**Operations.**\n",
    "\n",
    "* **SET(rank, $w$, $y$)**: add $(w,y)$ to the leaf at `rank`; bubble updates to the root, maintaining subtree aggregates $(W,Y)$. Cost $O(\\log n)$.\n",
    "* **SEARCH($t$)**: given a weight target $t=\\alpha \\cdot W_{\\text{total}}$, descend from the root choosing left/right by comparing $t$ to the left child’s weight. This finds the **leaf** where the weighted rank crosses $t$; along the way you accumulate the **prefix** $W^-$ and $Y^-$. Return $(W^-, Y^-, q)$ where $q$ is the leaf’s $y$-value (the current weighted $\\alpha$-quantile). Cost $O(\\log n)$.\n",
    "\n",
    "(A symmetric right→left sweep yields right-child losses; add left+right to score each threshold.)\n",
    "\n",
    "**Complexity and performance.**\n",
    "\n",
    "* Each step performs one `SET` and one `SEARCH`: **$O(\\log n)$** each ⇒ **$O(n\\log n)$** per sweep, worst-case by design.\n",
    "* In practice this variant is typically **~4× slower** than the two-heaps method on typical data (larger constants, more memory traffic), but it provides a clean upper bound and deterministic per-step work.\n",
    "\n",
    "**Implementation.**\n",
    "\n",
    "* The weighted segment tree class: `lib/ds/segment_tree.py` \n",
    "* The left→right sweep loop: `lib/algos/segment_tree.py` \n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Fenwick tree (Binary Indexed Tree, guaranteed $O(n\\log n)$)\n",
    "\n",
    "**Idea.**\n",
    "The Fenwick tree variant offers a simpler, cache-friendly structure that stores prefix sums in arrays rather than a binary node hierarchy.\n",
    "Two parallel BITs are maintained over the **ranks of $y$**:\n",
    "\n",
    "* `tree_w` for cumulative weights $\\sum w_i$, and  \n",
    "* `tree_wy` for cumulative weighted targets $\\sum w_i y_i$.\n",
    "\n",
    "Before the sweep, compute `rank[i]` as the index of $y_i$ in sorted order.\n",
    "During the left→right sweep over the candidate feature:\n",
    "\n",
    "1. **ADD(rank, $w$, $y$)**: point update at `rank` in both trees. Cost $O(\\log n)$.\n",
    "2. **SEARCH($t$)**: binary search on cumulative weight for $t=\\alpha \\cdot W_{\\text{total}}$ to find the weighted $\\alpha$-quantile rank.  \n",
    "   Retrieve the prefix aggregates $(W^-, Y^-)$ and the corresponding value $q$. Cost $O(\\log n)$.\n",
    "3. Compute the loss using the same $O(1)$ formula as before.\n",
    "\n",
    "**Complexity and performance.**\n",
    "\n",
    "* Each step performs one `ADD` and one `SEARCH`: both $O(\\log n)$ ⇒ total **$O(n\\log n)$** per sweep (worst-case guaranteed).\n",
    "* In practice, this variant has similar performances than the two-heaps method on typical data,\n",
    "  faster than the segment tree due to contiguous memory and smaller constants.\n",
    "\n",
    "**Implementation:**\n",
    "- Weighted Fenwick tree class: `lib/ds/fenwick.py`\n",
    "- Left→right sweep logic: `lib/algos/fenwick.py`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33aba0",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Complexity of the two-heaps algorithm\n",
    "\n",
    "See the notebook [complexity_experiments](https://github.com/cakedev0/fast-mae-split/blob/main/complexity_experiments.ipynb) for experiments and plots illustrating the different statements of this section.\n",
    "\n",
    "### 3.1 Worst-case for two-heaps\n",
    "\n",
    "In adversarial orders of weights, a single insertion can force the $\\alpha$-quantile boundary to traverse $\\Theta(n)$ items (each move is a heap pop+push, $O(\\log n)$). Thus **worst-case** per insertion is $O(n\\log n)$ and a full sweep can be **$O(n^2\\log n)$**.\n",
    "\n",
    "This does **not** occur in typical data; it requires systematically placing extreme weights to repeatedly push the boundary across many tiny items.\n",
    "\n",
    "\n",
    "### 3.2 Expected $O(n\\log n)$ per feature — two versions\n",
    "\n",
    "We give two simple sets of conditions under which the **expected** number of boundary moves per insertion is **$O(1)$**, yielding $O(n\\log n)$ total expected time (insert + moves, each $O(\\log n)$).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2A Independence model: $Y \\perp W$\n",
    "\n",
    "**Assumptions.**\n",
    "\n",
    "* $Y_1, \\dots, Y_n\\;$ i.i.d. from a continuous distribution (or consistent tie-breaking).\n",
    "* $W_1, \\dots, W_n\\;$ i.i.d., non-negative, independent of the $Y_i$, with $0<\\mu=\\mathbb{E}[W_i]<\\infty$.\n",
    "\n",
    "**Intuition.** Insert $(Y_{t+1},W_{t+1})$. The target balance changes by at most the new weight, so the boundary must “absorb” weight\n",
    "$$\n",
    "B_t\\in{\\alpha W_{t+1}, (1-\\alpha)W_{t+1}}\\le W_{t+1}.\n",
    "$$\n",
    "\n",
    "To re-balance, we move boundary items one by one. By independence and continuity, the weights encountered at the boundary look like fresh draws from $W$, so a **typical boundary item contributes $\\approx \\mu$ weight**. Hence the expected number of moves is about $\\mathbb{E}[B_t]/\\mu\\le \\mu/\\mu=O(1)$ (up to a constant for the final overshoot). Each move costs $O(\\log n)$; adding the $O(\\log n)$ insertion yields $O(\\log n)$ expected time per sample and $O(n\\log n)$ per feature.\n",
    "\n",
    "**Remark.** Heavy tails with finite mean (e.g., log-normal, Pareto with $\\alpha>1$) increase variance but not the expectation; infinite-mean tails would break the bound. However, in practice, even with infinite-mean Pareto distributions, performance does not degrade significantly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2B Lower-bound model: $f(y)=\\mathbb{E}[W\\mid Y=y]\\ge c>0$\n",
    "\n",
    "**Assumptions.**\n",
    "\n",
    "* $(Y_i,W_i)$ are i.i.d.; $\\mu=\\mathbb{E}[W_i]<\\infty$.\n",
    "* $Y$ has a continuous distribution (or consistent tie-breaking).\n",
    "* There exists $c>0$ such that for all $y$ in the support of $Y$,\n",
    "  $$\n",
    "  f(y):=\\mathbb{E}[W\\mid Y=y]\\ \\ge\\ c.\n",
    "  $$\n",
    "\n",
    "**Intuition.** As above, each insertion requires shifting at most $B_t\\le W_{t+1}$ weight across the boundary. The items encountered while sliding the boundary have an **expected weight of at least $c$**. Therefore, the **expected number of moves** satisfies\n",
    "$$\n",
    "\\mathbb{E}[K_t]\\ \\lesssim\\ \\frac{\\mathbb{E}[B_t]}{c} + 1 \\ \\le\\ \\frac{\\mu}{c}+1 \\ =\\ O(1).\n",
    "$$\n",
    "\n",
    "Again, with $O(\\log n)$ per move and insertion, we obtain $O(n\\log n)$ expected time, but with possibly a much bigger constant than above, if $\\frac{\\mu}{c}$ is big. See graph *small_weights_around_median* in `complexity_experiments.ipynb` for an example of such a setting.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Empirical performance\n",
    "\n",
    "See [complexity_experiments](https://github.com/cakedev0/fast-mae-split/blob/main/complexity_experiments.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Related work (brief)\n",
    "\n",
    "This approach adapts the classic two-heaps streaming median to **weighted $\\alpha$-quantiles** and couples it with a simple $O(1)$ pinball-loss evaluation. Some [prior work](https://faculty.ucmerced.edu/hbhat/BhatKumarVaz_final.pdf) also proposes a two-heaps algorithm for quantile-regression trees, but it omits weights and rely on more elaborate derivations for pinball-loss evaluation ($O(1)$ too, but might not be easy to adapt for the weighted case). The present formulation is simple and easy to integrate into open-source libraries such as scikit-learn.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
